\chapter{Embedding of Graph Structure Information}\label{chap:1}

\section{Node2Vec Theoretical Study}

\subsection{Random Walks Generation}
\subsection{Word2Vec}

\subsection{Parameters Description}

\subsection{Reference Implementation Caveats}
RAM Consumption
SPARK implementation is outdated, it approximates embeding and introduces limit of maximum node degree and still stores precomputed transition probablities. 



% Оценить вероятность появления слова $w_3$ после слов $w_1$ и $w_2$, идущих подряд, можно при помощи формулы \ref{eq:likelihood}:

% \begin{equation}
%   p(w_3|w_1,w_2) = \frac{f(w_1, w_2, w_3)}{f(w_1, w_2)},
%   \label{eq:likelihood}
% \end{equation}
% %
% где $f(w_1, w_2, w_3)$ --- частота появления триграммы $(w_1, w_2, w_3)$ в корпусе, $f(w_1, w_2)$ --- частота появления биграммы $(w_1, w_2)$.

\section{Conclusions on chapter 1}
In theory node2vec is cool, but it lacks truly distributed implementation
