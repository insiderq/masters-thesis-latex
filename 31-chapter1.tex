\chapter{node2vec Theoretical Study}\label{chap:1}
In this chapter we provide detailed description on node2vec embedding method. First, we get through exact process of embedding generation, then we outline algorithm hyperparameters. Last, we give an overview of existing algorithm implementations.

\section{Embedding Generation}
\subsection{Random Walks Generation}
\subsection{Word2Vec}

\section{Parameters Description}

\section{Reference Implementation Caveats}
RAM Consumption
SPARK implementation is outdated, it approximates embeding and introduces limit of maximum node degree and still stores precomputed transition probablities. 



% Оценить вероятность появления слова $w_3$ после слов $w_1$ и $w_2$, идущих подряд, можно при помощи формулы \ref{eq:likelihood}:

% \begin{equation}
%   p(w_3|w_1,w_2) = \frac{f(w_1, w_2, w_3)}{f(w_1, w_2)},
%   \label{eq:likelihood}
% \end{equation}
% %
% где $f(w_1, w_2, w_3)$ --- частота появления триграммы $(w_1, w_2, w_3)$ в корпусе, $f(w_1, w_2)$ --- частота появления биграммы $(w_1, w_2)$.

\section{Conclusions on chapter 1}
In theory node2vec is cool, but it lacks truly distributed implementation
